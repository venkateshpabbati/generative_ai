{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2aaf648-fc8e-4d99-a204-0f0e74071337",
   "metadata": {},
   "source": [
    "# Download dataset from the below kaggle link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed565a3d-b247-43a5-a859-b43176457e4e",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c6c1e57-1daf-46d8-ad2c-9a6485fe844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db7b601-b559-4016-9cb7-080a0171636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b952b145-f50e-422a-8e61-c89b1f503872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "imdb = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c328aef9-acb4-4d9b-9e1f-6c73ef4f7647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a471c2-8dbb-472e-9bfc-9a4942639081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  One of the other reviewers has mentioned that ...\n",
       "1  A wonderful little production. <br /><br />The...\n",
       "2  I thought this was a wonderful way to spend ti...\n",
       "3  Basically there's a family where a little boy ...\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = imdb.iloc[:5,:1].copy()\n",
    "df_test = imdb.iloc[5:10,:1].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f9e3855-248a-4b60-81a9-26fd78593a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "5  Probably my all-time favorite movie, a story o...\n",
       "6  I sure would like to see a resurrection of a u...\n",
       "7  This show was an amazing, fresh & innovative i...\n",
       "8  Encouraged by the positive comments about this...\n",
       "9  If you like original gut wrenching laughter yo..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fc349-5ef0-46cf-8170-34566b100582",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1aa1da-fd80-4ba5-9862-e4b8a82be45d",
   "metadata": {},
   "source": [
    "### Stopwords Removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b1e183-6ef3-4800-8fbf-02e65adb988a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>[One, reviewers, mentioned, watching, 1, Oz, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>[wonderful, little, production, ., &lt;, br, /, &gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>[Basically, 's, family, little, boy, (, Jake, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>[Petter, Mattei, 's, ``, Love, Time, Money, ''...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                        no_stopwords  \n",
       "0  [One, reviewers, mentioned, watching, 1, Oz, e...  \n",
       "1  [wonderful, little, production, ., <, br, /, >...  \n",
       "2  [thought, wonderful, way, spend, time, hot, su...  \n",
       "3  [Basically, 's, family, little, boy, (, Jake, ...  \n",
       "4  [Petter, Mattei, 's, ``, Love, Time, Money, ''...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['no_stopwords'] = df['review'].apply(lambda x: [word for word in word_tokenize(x) if word.lower() not in stop_words])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea595393-2bfc-4395-bd31-d9f66cac45f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One reviewers mentioned watching 1 Oz episode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>wonderful little production . &lt; br / &gt; &lt; br / ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>Basically 's family little boy ( Jake ) thinks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>Petter Mattei 's `` Love Time Money '' visuall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                        no_stopwords  \n",
       "0  One reviewers mentioned watching 1 Oz episode ...  \n",
       "1  wonderful little production . < br / > < br / ...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  Basically 's family little boy ( Jake ) thinks...  \n",
       "4  Petter Mattei 's `` Love Time Money '' visuall...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting back from list to string\n",
    "df['no_stopwords'] = df['no_stopwords'].apply(lambda x: ' '.join(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ce998-050f-4db5-bddd-ac803da9c433",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0dfd118-2e6b-4d20-8899-37420fb17c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One reviewers mentioned watching 1 Oz episode ...</td>\n",
       "      <td>[One, reviewers, mentioned, watching, 1, Oz, e...</td>\n",
       "      <td>[One reviewers mentioned watching 1 Oz episode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>wonderful little production . &lt; br / &gt; &lt; br / ...</td>\n",
       "      <td>[wonderful, little, production, ., &lt;, br, /, &gt;...</td>\n",
       "      <td>[wonderful little production ., &lt; br / &gt; &lt; br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>[thought wonderful way spend time hot summer w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>Basically 's family little boy ( Jake ) thinks...</td>\n",
       "      <td>[Basically, 's, family, little, boy, (, Jake, ...</td>\n",
       "      <td>[Basically 's family little boy ( Jake ) think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>Petter Mattei 's `` Love Time Money '' visuall...</td>\n",
       "      <td>[Petter, Mattei, 's, ``, Love, Time, Money, ``...</td>\n",
       "      <td>[Petter Mattei 's `` Love Time Money '' visual...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  One reviewers mentioned watching 1 Oz episode ...   \n",
       "1  wonderful little production . < br / > < br / ...   \n",
       "2  thought wonderful way spend time hot summer we...   \n",
       "3  Basically 's family little boy ( Jake ) thinks...   \n",
       "4  Petter Mattei 's `` Love Time Money '' visuall...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [One, reviewers, mentioned, watching, 1, Oz, e...   \n",
       "1  [wonderful, little, production, ., <, br, /, >...   \n",
       "2  [thought, wonderful, way, spend, time, hot, su...   \n",
       "3  [Basically, 's, family, little, boy, (, Jake, ...   \n",
       "4  [Petter, Mattei, 's, ``, Love, Time, Money, ``...   \n",
       "\n",
       "                                     sentence_tokens  \n",
       "0  [One reviewers mentioned watching 1 Oz episode...  \n",
       "1  [wonderful little production ., < br / > < br ...  \n",
       "2  [thought wonderful way spend time hot summer w...  \n",
       "3  [Basically 's family little boy ( Jake ) think...  \n",
       "4  [Petter Mattei 's `` Love Time Money '' visual...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "df['word_tokens'] = df['no_stopwords'].apply(word_tokenize)\n",
    "df['sentence_tokens'] = df['no_stopwords'].apply(sent_tokenize)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f675fc0-9af3-4dfd-b3ef-6a0ec4b80180",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26849ea6-3bb4-41ee-a94d-50cf4c2a0249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One reviewers mentioned watching 1 Oz episode ...</td>\n",
       "      <td>[One, reviewers, mentioned, watching, 1, Oz, e...</td>\n",
       "      <td>[One reviewers mentioned watching 1 Oz episode...</td>\n",
       "      <td>[one, review, mention, watch, 1, oz, episod, '...</td>\n",
       "      <td>[One, reviewer, mentioned, watching, 1, Oz, ep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>wonderful little production . &lt; br / &gt; &lt; br / ...</td>\n",
       "      <td>[wonderful, little, production, ., &lt;, br, /, &gt;...</td>\n",
       "      <td>[wonderful little production ., &lt; br / &gt; &lt; br ...</td>\n",
       "      <td>[wonder, littl, product, ., &lt;, br, /, &gt;, &lt;, br...</td>\n",
       "      <td>[wonderful, little, production, ., &lt;, br, /, &gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>[thought wonderful way spend time hot summer w...</td>\n",
       "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>Basically 's family little boy ( Jake ) thinks...</td>\n",
       "      <td>[Basically, 's, family, little, boy, (, Jake, ...</td>\n",
       "      <td>[Basically 's family little boy ( Jake ) think...</td>\n",
       "      <td>[basic, 's, famili, littl, boy, (, jake, ), th...</td>\n",
       "      <td>[Basically, 's, family, little, boy, (, Jake, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>Petter Mattei 's `` Love Time Money '' visuall...</td>\n",
       "      <td>[Petter, Mattei, 's, ``, Love, Time, Money, ``...</td>\n",
       "      <td>[Petter Mattei 's `` Love Time Money '' visual...</td>\n",
       "      <td>[petter, mattei, 's, ``, love, time, money, ``...</td>\n",
       "      <td>[Petter, Mattei, 's, ``, Love, Time, Money, ``...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  One reviewers mentioned watching 1 Oz episode ...   \n",
       "1  wonderful little production . < br / > < br / ...   \n",
       "2  thought wonderful way spend time hot summer we...   \n",
       "3  Basically 's family little boy ( Jake ) thinks...   \n",
       "4  Petter Mattei 's `` Love Time Money '' visuall...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [One, reviewers, mentioned, watching, 1, Oz, e...   \n",
       "1  [wonderful, little, production, ., <, br, /, >...   \n",
       "2  [thought, wonderful, way, spend, time, hot, su...   \n",
       "3  [Basically, 's, family, little, boy, (, Jake, ...   \n",
       "4  [Petter, Mattei, 's, ``, Love, Time, Money, ``...   \n",
       "\n",
       "                                     sentence_tokens  \\\n",
       "0  [One reviewers mentioned watching 1 Oz episode...   \n",
       "1  [wonderful little production ., < br / > < br ...   \n",
       "2  [thought wonderful way spend time hot summer w...   \n",
       "3  [Basically 's family little boy ( Jake ) think...   \n",
       "4  [Petter Mattei 's `` Love Time Money '' visual...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [one, review, mention, watch, 1, oz, episod, '...   \n",
       "1  [wonder, littl, product, ., <, br, /, >, <, br...   \n",
       "2  [thought, wonder, way, spend, time, hot, summe...   \n",
       "3  [basic, 's, famili, littl, boy, (, jake, ), th...   \n",
       "4  [petter, mattei, 's, ``, love, time, money, ``...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [One, reviewer, mentioned, watching, 1, Oz, ep...  \n",
       "1  [wonderful, little, production, ., <, br, /, >...  \n",
       "2  [thought, wonderful, way, spend, time, hot, su...  \n",
       "3  [Basically, 's, family, little, boy, (, Jake, ...  \n",
       "4  [Petter, Mattei, 's, ``, Love, Time, Money, ``...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming and Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['stemmed'] = df['word_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "df['lemmatized'] = df['word_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43f458-e133-4536-9772-5b3135ba88e8",
   "metadata": {},
   "source": [
    "### POS Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12112620-053c-45ae-acf1-93aa5d309e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One reviewers mentioned watching 1 Oz episode ...</td>\n",
       "      <td>[One, reviewers, mentioned, watching, 1, Oz, e...</td>\n",
       "      <td>[One reviewers mentioned watching 1 Oz episode...</td>\n",
       "      <td>[one, review, mention, watch, 1, oz, episod, '...</td>\n",
       "      <td>[One, reviewer, mentioned, watching, 1, Oz, ep...</td>\n",
       "      <td>[(One, CD), (reviewers, NNS), (mentioned, VBD)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>wonderful little production . &lt; br / &gt; &lt; br / ...</td>\n",
       "      <td>[wonderful, little, production, ., &lt;, br, /, &gt;...</td>\n",
       "      <td>[wonderful little production ., &lt; br / &gt; &lt; br ...</td>\n",
       "      <td>[wonder, littl, product, ., &lt;, br, /, &gt;, &lt;, br...</td>\n",
       "      <td>[wonderful, little, production, ., &lt;, br, /, &gt;...</td>\n",
       "      <td>[(wonderful, JJ), (little, JJ), (production, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>[thought wonderful way spend time hot summer w...</td>\n",
       "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>[(thought, VBN), (wonderful, JJ), (way, NN), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>Basically 's family little boy ( Jake ) thinks...</td>\n",
       "      <td>[Basically, 's, family, little, boy, (, Jake, ...</td>\n",
       "      <td>[Basically 's family little boy ( Jake ) think...</td>\n",
       "      <td>[basic, 's, famili, littl, boy, (, jake, ), th...</td>\n",
       "      <td>[Basically, 's, family, little, boy, (, Jake, ...</td>\n",
       "      <td>[(Basically, NNP), ('s, POS), (family, NN), (l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>Petter Mattei 's `` Love Time Money '' visuall...</td>\n",
       "      <td>[Petter, Mattei, 's, ``, Love, Time, Money, ``...</td>\n",
       "      <td>[Petter Mattei 's `` Love Time Money '' visual...</td>\n",
       "      <td>[petter, mattei, 's, ``, love, time, money, ``...</td>\n",
       "      <td>[Petter, Mattei, 's, ``, Love, Time, Money, ``...</td>\n",
       "      <td>[(Petter, NNP), (Mattei, NNP), ('s, POS), (``,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  One reviewers mentioned watching 1 Oz episode ...   \n",
       "1  wonderful little production . < br / > < br / ...   \n",
       "2  thought wonderful way spend time hot summer we...   \n",
       "3  Basically 's family little boy ( Jake ) thinks...   \n",
       "4  Petter Mattei 's `` Love Time Money '' visuall...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [One, reviewers, mentioned, watching, 1, Oz, e...   \n",
       "1  [wonderful, little, production, ., <, br, /, >...   \n",
       "2  [thought, wonderful, way, spend, time, hot, su...   \n",
       "3  [Basically, 's, family, little, boy, (, Jake, ...   \n",
       "4  [Petter, Mattei, 's, ``, Love, Time, Money, ``...   \n",
       "\n",
       "                                     sentence_tokens  \\\n",
       "0  [One reviewers mentioned watching 1 Oz episode...   \n",
       "1  [wonderful little production ., < br / > < br ...   \n",
       "2  [thought wonderful way spend time hot summer w...   \n",
       "3  [Basically 's family little boy ( Jake ) think...   \n",
       "4  [Petter Mattei 's `` Love Time Money '' visual...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [one, review, mention, watch, 1, oz, episod, '...   \n",
       "1  [wonder, littl, product, ., <, br, /, >, <, br...   \n",
       "2  [thought, wonder, way, spend, time, hot, summe...   \n",
       "3  [basic, 's, famili, littl, boy, (, jake, ), th...   \n",
       "4  [petter, mattei, 's, ``, love, time, money, ``...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [One, reviewer, mentioned, watching, 1, Oz, ep...   \n",
       "1  [wonderful, little, production, ., <, br, /, >...   \n",
       "2  [thought, wonderful, way, spend, time, hot, su...   \n",
       "3  [Basically, 's, family, little, boy, (, Jake, ...   \n",
       "4  [Petter, Mattei, 's, ``, Love, Time, Money, ``...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  [(One, CD), (reviewers, NNS), (mentioned, VBD)...  \n",
       "1  [(wonderful, JJ), (little, JJ), (production, N...  \n",
       "2  [(thought, VBN), (wonderful, JJ), (way, NN), (...  \n",
       "3  [(Basically, NNP), ('s, POS), (family, NN), (l...  \n",
       "4  [(Petter, NNP), (Mattei, NNP), ('s, POS), (``,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS Tagging\n",
    "df['pos_tags'] = df['word_tokens'].apply(pos_tag)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d03bea-a968-4c30-805c-80f65d6c2d0e",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c69a4-d5be-447c-9639-9afc86886eee",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd53078c-888d-4017-9425-2ea769c2d243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>accustomed</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actors</th>\n",
       "      <th>addiction</th>\n",
       "      <th>adrian</th>\n",
       "      <th>agenda</th>\n",
       "      <th>agreements</th>\n",
       "      <th>air</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062527</td>\n",
       "      <td>0.062527</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100558</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100558</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10804</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.098464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.196928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         10  accustomed    acting    action    actors  addiction    adrian  \\\n",
       "0  0.000000    0.062527  0.000000  0.000000  0.000000    0.00000  0.000000   \n",
       "1  0.000000    0.000000  0.000000  0.000000  0.100558    0.00000  0.000000   \n",
       "2  0.000000    0.000000  0.000000  0.000000  0.000000    0.10804  0.000000   \n",
       "3  0.098464    0.000000  0.000000  0.000000  0.000000    0.00000  0.000000   \n",
       "4  0.000000    0.000000  0.072891  0.072891  0.000000    0.00000  0.072891   \n",
       "\n",
       "     agenda  agreements      air  ...      word      work     world     worth  \\\n",
       "0  0.062527    0.062527  0.00000  ...  0.125053  0.000000  0.000000  0.000000   \n",
       "1  0.000000    0.000000  0.00000  ...  0.000000  0.000000  0.000000  0.100558   \n",
       "2  0.000000    0.000000  0.10804  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000    0.000000  0.00000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000    0.000000  0.00000  ...  0.000000  0.072891  0.072891  0.000000   \n",
       "\n",
       "      would   written    years      york    young    zombie  \n",
       "0  0.125053  0.000000  0.00000  0.000000  0.00000  0.000000  \n",
       "1  0.000000  0.100558  0.00000  0.000000  0.00000  0.000000  \n",
       "2  0.000000  0.000000  0.10804  0.000000  0.10804  0.000000  \n",
       "3  0.000000  0.000000  0.00000  0.000000  0.00000  0.196928  \n",
       "4  0.000000  0.000000  0.00000  0.072891  0.00000  0.000000  \n",
       "\n",
       "[5 rows x 403 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(df['no_stopwords'])\n",
    "tfidf_df = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702607f8-0f86-4938-b08b-1020b25e72ba",
   "metadata": {},
   "source": [
    "### One-Hot Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a17c5a-9f84-4191-99ad-f3522b2a5da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_stopwords_Basically 's family little boy ( Jake ) thinks 's zombie closet &amp; parents fighting time. &lt; br / &gt; &lt; br / &gt; movie slower soap opera ... suddenly , Jake decides become Rambo kill zombie. &lt; br / &gt; &lt; br / &gt; OK , first 're going make film must Decide thriller drama ! drama movie watchable . Parents divorcing &amp; arguing like real life . Jake closet totally ruins film ! expected see BOOGEYMAN similar movie , instead watched drama meaningless thriller spots. &lt; br / &gt; &lt; br / &gt; 3 10 well playing parents &amp; descent dialogs . shots Jake : ignore .</th>\n",
       "      <th>no_stopwords_One reviewers mentioned watching 1 Oz episode 'll hooked . right , exactly happened me. &lt; br / &gt; &lt; br / &gt; first thing struck Oz brutality unflinching scenes violence , set right word GO . Trust , show faint hearted timid . show pulls punches regards drugs , sex violence . hardcore , classic use word. &lt; br / &gt; &lt; br / &gt; called OZ nickname given Oswald Maximum Security State Penitentary . focuses mainly Emerald City , experimental section prison cells glass fronts face inwards , privacy high agenda . Em City home many .. Aryans , Muslims , gangstas , Latinos , Christians , Italians , Irish .... scuffles , death stares , dodgy dealings shady agreements never far away. &lt; br / &gt; &lt; br / &gt; would say main appeal show due fact goes shows would n't dare . Forget pretty pictures painted mainstream audiences , forget charm , forget romance ... OZ n't mess around . first episode ever saw struck nasty surreal , could n't say ready , watched , developed taste Oz , got accustomed high levels graphic violence . violence , injustice ( crooked guards 'll sold nickel , inmates 'll kill order get away , well mannered , middle class inmates turned prison bitches due lack street skills prison experience ) Watching Oz , may become comfortable uncomfortable viewing .... thats get touch darker side .</th>\n",
       "      <th>no_stopwords_Petter Mattei 's `` Love Time Money '' visually stunning film watch . Mr. Mattei offers us vivid portrait human relations . movie seems telling us money , power success people different situations encounter . &lt; br / &gt; &lt; br / &gt; variation Arthur Schnitzler 's play theme , director transfers action present time New York different characters meet connect . one connected one way , another next person , one seems know previous point contact . Stylishly , film sophisticated luxurious look . taken see people live world live habitat. &lt; br / &gt; &lt; br / &gt; thing one gets souls picture different stages loneliness one inhabits . big city exactly best place human relations find sincere fulfillment , one discerns case people encounter. &lt; br / &gt; &lt; br / &gt; acting good Mr. Mattei 's direction . Steve Buscemi , Rosario Dawson , Carol Kane , Michael Imperioli , Adrian Grenier , rest talented cast , make characters come alive. &lt; br / &gt; &lt; br / &gt; wish Mr. Mattei good luck await anxiously next work .</th>\n",
       "      <th>no_stopwords_thought wonderful way spend time hot summer weekend , sitting air conditioned theater watching light-hearted comedy . plot simplistic , dialogue witty characters likable ( even well bread suspected serial killer ) . may disappointed realize Match Point 2 : Risk Addiction , thought proof Woody Allen still fully control style many us grown love. &lt; br / &gt; &lt; br / &gt; 'd laughed one Woody 's comedies years ( dare say decade ? ) . 've never impressed Scarlet Johanson , managed tone `` sexy '' image jumped right average , spirited young woman. &lt; br / &gt; &lt; br / &gt; may crown jewel career , wittier `` Devil Wears Prada '' interesting `` Superman '' great comedy go see friends .</th>\n",
       "      <th>no_stopwords_wonderful little production . &lt; br / &gt; &lt; br / &gt; filming technique unassuming- old-time-BBC fashion gives comforting , sometimes discomforting , sense realism entire piece . &lt; br / &gt; &lt; br / &gt; actors extremely well chosen- Michael Sheen `` got polari '' voices pat ! truly see seamless editing guided references Williams ' diary entries , well worth watching terrificly written performed piece . masterful production one great master 's comedy life . &lt; br / &gt; &lt; br / &gt; realism really comes home little things : fantasy guard , rather use traditional 'dream ' techniques remains solid disappears . plays knowledge senses , particularly scenes concerning Orton Halliwell sets ( particularly flat Halliwell 's murals decorating every surface ) terribly well done .</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_stopwords_Basically 's family little boy ( Jake ) thinks 's zombie closet & parents fighting time. < br / > < br / > movie slower soap opera ... suddenly , Jake decides become Rambo kill zombie. < br / > < br / > OK , first 're going make film must Decide thriller drama ! drama movie watchable . Parents divorcing & arguing like real life . Jake closet totally ruins film ! expected see BOOGEYMAN similar movie , instead watched drama meaningless thriller spots. < br / > < br / > 3 10 well playing parents & descent dialogs . shots Jake : ignore .  \\\n",
       "0                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "2                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "3                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "\n",
       "   no_stopwords_One reviewers mentioned watching 1 Oz episode 'll hooked . right , exactly happened me. < br / > < br / > first thing struck Oz brutality unflinching scenes violence , set right word GO . Trust , show faint hearted timid . show pulls punches regards drugs , sex violence . hardcore , classic use word. < br / > < br / > called OZ nickname given Oswald Maximum Security State Penitentary . focuses mainly Emerald City , experimental section prison cells glass fronts face inwards , privacy high agenda . Em City home many .. Aryans , Muslims , gangstas , Latinos , Christians , Italians , Irish .... scuffles , death stares , dodgy dealings shady agreements never far away. < br / > < br / > would say main appeal show due fact goes shows would n't dare . Forget pretty pictures painted mainstream audiences , forget charm , forget romance ... OZ n't mess around . first episode ever saw struck nasty surreal , could n't say ready , watched , developed taste Oz , got accustomed high levels graphic violence . violence , injustice ( crooked guards 'll sold nickel , inmates 'll kill order get away , well mannered , middle class inmates turned prison bitches due lack street skills prison experience ) Watching Oz , may become comfortable uncomfortable viewing .... thats get touch darker side .  \\\n",
       "0                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "1                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "2                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "\n",
       "   no_stopwords_Petter Mattei 's `` Love Time Money '' visually stunning film watch . Mr. Mattei offers us vivid portrait human relations . movie seems telling us money , power success people different situations encounter . < br / > < br / > variation Arthur Schnitzler 's play theme , director transfers action present time New York different characters meet connect . one connected one way , another next person , one seems know previous point contact . Stylishly , film sophisticated luxurious look . taken see people live world live habitat. < br / > < br / > thing one gets souls picture different stages loneliness one inhabits . big city exactly best place human relations find sincere fulfillment , one discerns case people encounter. < br / > < br / > acting good Mr. Mattei 's direction . Steve Buscemi , Rosario Dawson , Carol Kane , Michael Imperioli , Adrian Grenier , rest talented cast , make characters come alive. < br / > < br / > wish Mr. Mattei good luck await anxiously next work .  \\\n",
       "0                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "2                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "3                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "4                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "\n",
       "   no_stopwords_thought wonderful way spend time hot summer weekend , sitting air conditioned theater watching light-hearted comedy . plot simplistic , dialogue witty characters likable ( even well bread suspected serial killer ) . may disappointed realize Match Point 2 : Risk Addiction , thought proof Woody Allen still fully control style many us grown love. < br / > < br / > 'd laughed one Woody 's comedies years ( dare say decade ? ) . 've never impressed Scarlet Johanson , managed tone `` sexy '' image jumped right average , spirited young woman. < br / > < br / > may crown jewel career , wittier `` Devil Wears Prada '' interesting `` Superman '' great comedy go see friends .  \\\n",
       "0                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "2                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "3                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "   no_stopwords_wonderful little production . < br / > < br / > filming technique unassuming- old-time-BBC fashion gives comforting , sometimes discomforting , sense realism entire piece . < br / > < br / > actors extremely well chosen- Michael Sheen `` got polari '' voices pat ! truly see seamless editing guided references Williams ' diary entries , well worth watching terrificly written performed piece . masterful production one great master 's comedy life . < br / > < br / > realism really comes home little things : fantasy guard , rather use traditional 'dream ' techniques remains solid disappears . plays knowledge senses , particularly scenes concerning Orton Halliwell sets ( particularly flat Halliwell 's murals decorating every surface ) terribly well done .  \n",
       "0                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "1                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "2                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_encoded = encoder.fit_transform(df[['no_stopwords']])\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out())\n",
    "one_hot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11c63d-0a36-49b4-a0e2-9f16a30f0959",
   "metadata": {},
   "source": [
    "### Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3051b565-88bc-4bdb-a21f-8c402c8030d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>accustomed</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actors</th>\n",
       "      <th>addiction</th>\n",
       "      <th>adrian</th>\n",
       "      <th>agenda</th>\n",
       "      <th>agreements</th>\n",
       "      <th>air</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  accustomed  acting  action  actors  addiction  adrian  agenda  \\\n",
       "0   0           1       0       0       0          0       0       1   \n",
       "1   0           0       0       0       1          0       0       0   \n",
       "2   0           0       0       0       0          1       0       0   \n",
       "3   1           0       0       0       0          0       0       0   \n",
       "4   0           0       1       1       0          0       1       0   \n",
       "\n",
       "   agreements  air  ...  word  work  world  worth  would  written  years  \\\n",
       "0           1    0  ...     2     0      0      0      2        0      0   \n",
       "1           0    0  ...     0     0      0      1      0        1      0   \n",
       "2           0    1  ...     0     0      0      0      0        0      1   \n",
       "3           0    0  ...     0     0      0      0      0        0      0   \n",
       "4           0    0  ...     0     1      1      0      0        0      0   \n",
       "\n",
       "   york  young  zombie  \n",
       "0     0      0       0  \n",
       "1     0      0       0  \n",
       "2     0      1       0  \n",
       "3     0      0       2  \n",
       "4     1      0       0  \n",
       "\n",
       "[5 rows x 403 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_vectors = count_vectorizer.fit_transform(df['no_stopwords'])\n",
    "bow_df = pd.DataFrame(bow_vectors.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ed342-71eb-4629-b34d-8e717426349e",
   "metadata": {},
   "source": [
    "### Unigram, Bigram, n-gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "187265b9-4bda-4c19-865f-14d1ff53b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram, Bigram, n-gram\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "unigram_vectors = unigram_vectorizer.fit_transform(df['no_stopwords'])\n",
    "bigram_vectors = bigram_vectorizer.fit_transform(df['no_stopwords'])\n",
    "ngram_vectors = ngram_vectorizer.fit_transform(df['no_stopwords'])\n",
    "\n",
    "unigram_df = pd.DataFrame(unigram_vectors.toarray(), columns=unigram_vectorizer.get_feature_names_out())\n",
    "bigram_df = pd.DataFrame(bigram_vectors.toarray(), columns=bigram_vectorizer.get_feature_names_out())\n",
    "ngram_df = pd.DataFrame(ngram_vectors.toarray(), columns=ngram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf1f888d-0128-49ae-b0d9-293f525d08c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>accustomed</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actors</th>\n",
       "      <th>addiction</th>\n",
       "      <th>adrian</th>\n",
       "      <th>agenda</th>\n",
       "      <th>agreements</th>\n",
       "      <th>air</th>\n",
       "      <th>...</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  accustomed  acting  action  actors  addiction  adrian  agenda  \\\n",
       "0   0           1       0       0       0          0       0       1   \n",
       "1   0           0       0       0       1          0       0       0   \n",
       "2   0           0       0       0       0          1       0       0   \n",
       "3   1           0       0       0       0          0       0       0   \n",
       "4   0           0       1       1       0          0       1       0   \n",
       "\n",
       "   agreements  air  ...  word  work  world  worth  would  written  years  \\\n",
       "0           1    0  ...     2     0      0      0      2        0      0   \n",
       "1           0    0  ...     0     0      0      1      0        1      0   \n",
       "2           0    1  ...     0     0      0      0      0        0      1   \n",
       "3           0    0  ...     0     0      0      0      0        0      0   \n",
       "4           0    0  ...     0     1      1      0      0        0      0   \n",
       "\n",
       "   york  young  zombie  \n",
       "0     0      0       0  \n",
       "1     0      0       0  \n",
       "2     0      1       0  \n",
       "3     0      0       2  \n",
       "4     1      0       0  \n",
       "\n",
       "[5 rows x 403 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "427714ac-3ad1-4299-8b7d-84af32f17563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 well</th>\n",
       "      <th>accustomed high</th>\n",
       "      <th>acting good</th>\n",
       "      <th>action present</th>\n",
       "      <th>actors extremely</th>\n",
       "      <th>addiction thought</th>\n",
       "      <th>adrian grenier</th>\n",
       "      <th>agenda em</th>\n",
       "      <th>agreements never</th>\n",
       "      <th>air conditioned</th>\n",
       "      <th>...</th>\n",
       "      <th>world live</th>\n",
       "      <th>worth watching</th>\n",
       "      <th>would dare</th>\n",
       "      <th>would say</th>\n",
       "      <th>written performed</th>\n",
       "      <th>years dare</th>\n",
       "      <th>york different</th>\n",
       "      <th>young woman</th>\n",
       "      <th>zombie br</th>\n",
       "      <th>zombie closet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 536 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 well  accustomed high  acting good  action present  actors extremely  \\\n",
       "0        0                1            0               0                 0   \n",
       "1        0                0            0               0                 1   \n",
       "2        0                0            0               0                 0   \n",
       "3        1                0            0               0                 0   \n",
       "4        0                0            1               1                 0   \n",
       "\n",
       "   addiction thought  adrian grenier  agenda em  agreements never  \\\n",
       "0                  0               0          1                 1   \n",
       "1                  0               0          0                 0   \n",
       "2                  1               0          0                 0   \n",
       "3                  0               0          0                 0   \n",
       "4                  0               1          0                 0   \n",
       "\n",
       "   air conditioned  ...  world live  worth watching  would dare  would say  \\\n",
       "0                0  ...           0               0           1          1   \n",
       "1                0  ...           0               1           0          0   \n",
       "2                1  ...           0               0           0          0   \n",
       "3                0  ...           0               0           0          0   \n",
       "4                0  ...           1               0           0          0   \n",
       "\n",
       "   written performed  years dare  york different  young woman  zombie br  \\\n",
       "0                  0           0               0            0          0   \n",
       "1                  1           0               0            0          0   \n",
       "2                  0           1               0            1          0   \n",
       "3                  0           0               0            0          1   \n",
       "4                  0           0               1            0          0   \n",
       "\n",
       "   zombie closet  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              1  \n",
       "4              0  \n",
       "\n",
       "[5 rows x 536 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3a670a8-1258-4ad5-8042-93b8f07c405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>10 well</th>\n",
       "      <th>10 well playing</th>\n",
       "      <th>accustomed</th>\n",
       "      <th>accustomed high</th>\n",
       "      <th>accustomed high levels</th>\n",
       "      <th>acting</th>\n",
       "      <th>acting good</th>\n",
       "      <th>acting good mr</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>york different</th>\n",
       "      <th>york different characters</th>\n",
       "      <th>young</th>\n",
       "      <th>young woman</th>\n",
       "      <th>young woman br</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombie br</th>\n",
       "      <th>zombie br br</th>\n",
       "      <th>zombie closet</th>\n",
       "      <th>zombie closet parents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1488 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  10 well  10 well playing  accustomed  accustomed high  \\\n",
       "0   0        0                0           1                1   \n",
       "1   0        0                0           0                0   \n",
       "2   0        0                0           0                0   \n",
       "3   1        1                1           0                0   \n",
       "4   0        0                0           0                0   \n",
       "\n",
       "   accustomed high levels  acting  acting good  acting good mr  action  ...  \\\n",
       "0                       1       0            0               0       0  ...   \n",
       "1                       0       0            0               0       0  ...   \n",
       "2                       0       0            0               0       0  ...   \n",
       "3                       0       0            0               0       0  ...   \n",
       "4                       0       1            1               1       1  ...   \n",
       "\n",
       "   york different  york different characters  young  young woman  \\\n",
       "0               0                          0      0            0   \n",
       "1               0                          0      0            0   \n",
       "2               0                          0      1            1   \n",
       "3               0                          0      0            0   \n",
       "4               1                          1      0            0   \n",
       "\n",
       "   young woman br  zombie  zombie br  zombie br br  zombie closet  \\\n",
       "0               0       0          0             0              0   \n",
       "1               0       0          0             0              0   \n",
       "2               1       0          0             0              0   \n",
       "3               0       2          1             1              1   \n",
       "4               0       0          0             0              0   \n",
       "\n",
       "   zombie closet parents  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      1  \n",
       "4                      0  \n",
       "\n",
       "[5 rows x 1488 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "419667e9-d19e-4bf5-a8e0-a33f246f584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_and_feature_engineering(df_test):\n",
    "    # Stopwords Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df_test['no_stopwords'] = df_test['review'].apply(lambda x: [word for word in word_tokenize(x) if word.lower() not in stop_words])\n",
    "    df_test['no_stopwords'] = df_test['no_stopwords'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Tokenization\n",
    "    df_test['word_tokens'] = df_test['no_stopwords'].apply(word_tokenize)\n",
    "    df_test['sentence_tokens'] = df_test['no_stopwords'].apply(sent_tokenize)\n",
    "    \n",
    "    # Stemming and Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_test['stemmed'] = df_test['word_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    df_test['lemmatized'] = df_test['word_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "    \n",
    "    # POS Tagging\n",
    "    df_test['pos_tags'] = df_test['word_tokens'].apply(pos_tag)\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(df_test['no_stopwords'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # One-Hot Encoding\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_encoded = encoder.fit_transform(df_test[['no_stopwords']])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out())\n",
    "    \n",
    "    # Bag of Words\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    bow_vectors = count_vectorizer.fit_transform(df_test['no_stopwords'])\n",
    "    bow_df = pd.DataFrame(bow_vectors.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Unigram, Bigram, n-gram\n",
    "    unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "    ngram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "    \n",
    "    unigram_vectors = unigram_vectorizer.fit_transform(df_test['no_stopwords'])\n",
    "    bigram_vectors = bigram_vectorizer.fit_transform(df_test['no_stopwords'])\n",
    "    ngram_vectors = ngram_vectorizer.fit_transform(df_test['no_stopwords'])\n",
    "    \n",
    "    unigram_df = pd.DataFrame(unigram_vectors.toarray(), columns=unigram_vectorizer.get_feature_names_out())\n",
    "    bigram_df = pd.DataFrame(bigram_vectors.toarray(), columns=bigram_vectorizer.get_feature_names_out())\n",
    "    ngram_df = pd.DataFrame(ngram_vectors.toarray(), columns=ngram_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return df_test, tfidf_df, one_hot_df, bow_df, unigram_df, bigram_df, ngram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cfeed33-b309-4f06-8fa2-1d19b8f5f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_res, tfidf_df_res, one_hot_df_res, bow_df_res, unigram_df_res, bigram_df_res, ngram_df_res = preprocessing_and_feature_engineering(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2055b156-87d6-43cd-9cd2-357d996f5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>Probably all-time favorite movie , story selfl...</td>\n",
       "      <td>[Probably, all-time, favorite, movie, ,, story...</td>\n",
       "      <td>[Probably all-time favorite movie , story self...</td>\n",
       "      <td>[probabl, all-tim, favorit, movi, ,, stori, se...</td>\n",
       "      <td>[Probably, all-time, favorite, movie, ,, story...</td>\n",
       "      <td>[(Probably, RB), (all-time, JJ), (favorite, JJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>sure would like see resurrection dated Seahunt...</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>[sure would like see resurrection dated Seahun...</td>\n",
       "      <td>[sure, would, like, see, resurrect, date, seah...</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>[(sure, RB), (would, MD), (like, VB), (see, VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>show amazing , fresh &amp; innovative idea 70 's f...</td>\n",
       "      <td>[show, amazing, ,, fresh, &amp;, innovative, idea,...</td>\n",
       "      <td>[show amazing , fresh &amp; innovative idea 70 's ...</td>\n",
       "      <td>[show, amaz, ,, fresh, &amp;, innov, idea, 70, 's,...</td>\n",
       "      <td>[show, amazing, ,, fresh, &amp;, innovative, idea,...</td>\n",
       "      <td>[(show, NN), (amazing, JJ), (,, ,), (fresh, JJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>Encouraged positive comments film looking forw...</td>\n",
       "      <td>[Encouraged, positive, comments, film, looking...</td>\n",
       "      <td>[Encouraged positive comments film looking for...</td>\n",
       "      <td>[encourag, posit, comment, film, look, forward...</td>\n",
       "      <td>[Encouraged, positive, comment, film, looking,...</td>\n",
       "      <td>[(Encouraged, VBN), (positive, JJ), (comments,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>like original gut wrenching laughter like movi...</td>\n",
       "      <td>[like, original, gut, wrenching, laughter, lik...</td>\n",
       "      <td>[like original gut wrenching laughter like mov...</td>\n",
       "      <td>[like, origin, gut, wrench, laughter, like, mo...</td>\n",
       "      <td>[like, original, gut, wrenching, laughter, lik...</td>\n",
       "      <td>[(like, IN), (original, JJ), (gut, NN), (wrenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "5  Probably my all-time favorite movie, a story o...   \n",
       "6  I sure would like to see a resurrection of a u...   \n",
       "7  This show was an amazing, fresh & innovative i...   \n",
       "8  Encouraged by the positive comments about this...   \n",
       "9  If you like original gut wrenching laughter yo...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "5  Probably all-time favorite movie , story selfl...   \n",
       "6  sure would like see resurrection dated Seahunt...   \n",
       "7  show amazing , fresh & innovative idea 70 's f...   \n",
       "8  Encouraged positive comments film looking forw...   \n",
       "9  like original gut wrenching laughter like movi...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "5  [Probably, all-time, favorite, movie, ,, story...   \n",
       "6  [sure, would, like, see, resurrection, dated, ...   \n",
       "7  [show, amazing, ,, fresh, &, innovative, idea,...   \n",
       "8  [Encouraged, positive, comments, film, looking...   \n",
       "9  [like, original, gut, wrenching, laughter, lik...   \n",
       "\n",
       "                                     sentence_tokens  \\\n",
       "5  [Probably all-time favorite movie , story self...   \n",
       "6  [sure would like see resurrection dated Seahun...   \n",
       "7  [show amazing , fresh & innovative idea 70 's ...   \n",
       "8  [Encouraged positive comments film looking for...   \n",
       "9  [like original gut wrenching laughter like mov...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "5  [probabl, all-tim, favorit, movi, ,, stori, se...   \n",
       "6  [sure, would, like, see, resurrect, date, seah...   \n",
       "7  [show, amaz, ,, fresh, &, innov, idea, 70, 's,...   \n",
       "8  [encourag, posit, comment, film, look, forward...   \n",
       "9  [like, origin, gut, wrench, laughter, like, mo...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "5  [Probably, all-time, favorite, movie, ,, story...   \n",
       "6  [sure, would, like, see, resurrection, dated, ...   \n",
       "7  [show, amazing, ,, fresh, &, innovative, idea,...   \n",
       "8  [Encouraged, positive, comment, film, looking,...   \n",
       "9  [like, original, gut, wrenching, laughter, lik...   \n",
       "\n",
       "                                            pos_tags  \n",
       "5  [(Probably, RB), (all-time, JJ), (favorite, JJ...  \n",
       "6  [(sure, RB), (would, MD), (like, VB), (see, VB...  \n",
       "7  [(show, NN), (amazing, JJ), (,, ,), (fresh, JJ...  \n",
       "8  [(Encouraged, VBN), (positive, JJ), (comments,...  \n",
       "9  [(like, IN), (original, JJ), (gut, NN), (wrenc...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "400485cc-6af2-4ac5-ae97-8e5d198922d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>1990</th>\n",
       "      <th>25</th>\n",
       "      <th>70</th>\n",
       "      <th>950</th>\n",
       "      <th>acting</th>\n",
       "      <th>adventure</th>\n",
       "      <th>air</th>\n",
       "      <th>aired</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worst</th>\n",
       "      <th>would</th>\n",
       "      <th>wrenching</th>\n",
       "      <th>writing</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109987</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.094837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.094837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094837</td>\n",
       "      <td>0.094837</td>\n",
       "      <td>0.076514</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.459083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094837</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091744</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183487</td>\n",
       "      <td>0.091744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.074018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091744</td>\n",
       "      <td>0.074018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         10        15      1990        25        70   950  acting  adventure  \\\n",
       "0  0.000000  0.136326  0.000000  0.136326  0.000000  0.00    0.00   0.000000   \n",
       "1  0.094837  0.000000  0.000000  0.000000  0.000000  0.00    0.00   0.094837   \n",
       "2  0.000000  0.000000  0.091744  0.000000  0.091744  0.00    0.00   0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.11    0.11   0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.00    0.00   0.000000   \n",
       "\n",
       "        air     aired  ...     white      work     world  worst     would  \\\n",
       "0  0.000000  0.000000  ...  0.000000  0.000000  0.109987   0.00  0.000000   \n",
       "1  0.000000  0.000000  ...  0.094837  0.094837  0.076514   0.00  0.459083   \n",
       "2  0.183487  0.091744  ...  0.000000  0.000000  0.000000   0.00  0.074018   \n",
       "3  0.000000  0.000000  ...  0.000000  0.000000  0.000000   0.11  0.000000   \n",
       "4  0.000000  0.000000  ...  0.000000  0.000000  0.000000   0.00  0.000000   \n",
       "\n",
       "   wrenching   writing     years       you     young  \n",
       "0   0.000000  0.000000  0.109987  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.094837  0.000000  \n",
       "2   0.000000  0.091744  0.074018  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4   0.226493  0.000000  0.000000  0.000000  0.226493  \n",
       "\n",
       "[5 rows x 241 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "469d00e3-09ac-4bda-9367-5064d48c10ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_stopwords_Encouraged positive comments film looking forward watching film . Bad mistake . 've seen 950+ films truly one worst - 's awful almost every way : editing , pacing , storyline , 'acting , ' soundtrack ( film 's song - lame country tune - played less four times ) . film looks cheap nasty boring extreme . Rarely happy see end credits film . &lt; br / &gt; &lt; br / &gt; thing prevents giving 1-score Harvey Keitel - far best performance least seems making bit effort . One Keitel obsessives .</th>\n",
       "      <th>no_stopwords_Probably all-time favorite movie , story selflessness , sacrifice dedication noble cause , 's preachy boring . never gets old , despite seen 15 times last 25 years . Paul Lukas ' performance brings tears eyes , Bette Davis , one truly sympathetic roles , delight . kids , grandma says , like `` dressed-up midgets '' children , makes fun watch . mother 's slow awakening 's happening world roof believable startling . dozen thumbs , 'd `` '' movie .</th>\n",
       "      <th>no_stopwords_like original gut wrenching laughter like movie . young old love movie , hell even mom liked it. &lt; br / &gt; &lt; br / &gt; Great Camp ! ! !</th>\n",
       "      <th>no_stopwords_show amazing , fresh &amp; innovative idea 70 's first aired . first 7 8 years brilliant , things dropped . 1990 , show really funny anymore , 's continued decline complete waste time today. &lt; br / &gt; &lt; br / &gt; 's truly disgraceful far show fallen . writing painfully bad , performances almost bad - mildly entertaining respite guest-hosts , show probably would n't still air . find hard believe creator hand-selected original cast also chose band hacks followed . one recognize brilliance see fit replace mediocrity ? felt must give 2 stars respect original cast made show huge success . , show awful . ca n't believe 's still air .</th>\n",
       "      <th>no_stopwords_sure would like see resurrection dated Seahunt series tech today would bring back kid excitement me.I grew black white TV Seahunt Gunsmoke hero 's every week.You vote comeback new sea hunt.We need change pace TV would work world water adventure.Oh way thank outlet like view many viewpoints TV many movies.So ole way believe 've got wan na say.Would nice read plus points sea hunt.If rhymes would 10 lines would let submit , leave doubt quit , must go lets .</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_stopwords_Encouraged positive comments film looking forward watching film . Bad mistake . 've seen 950+ films truly one worst - 's awful almost every way : editing , pacing , storyline , 'acting , ' soundtrack ( film 's song - lame country tune - played less four times ) . film looks cheap nasty boring extreme . Rarely happy see end credits film . < br / > < br / > thing prevents giving 1-score Harvey Keitel - far best performance least seems making bit effort . One Keitel obsessives .  \\\n",
       "0                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "2                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "3                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "   no_stopwords_Probably all-time favorite movie , story selflessness , sacrifice dedication noble cause , 's preachy boring . never gets old , despite seen 15 times last 25 years . Paul Lukas ' performance brings tears eyes , Bette Davis , one truly sympathetic roles , delight . kids , grandma says , like `` dressed-up midgets '' children , makes fun watch . mother 's slow awakening 's happening world roof believable startling . dozen thumbs , 'd `` '' movie .  \\\n",
       "0                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "2                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "3                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "\n",
       "   no_stopwords_like original gut wrenching laughter like movie . young old love movie , hell even mom liked it. < br / > < br / > Great Camp ! ! !  \\\n",
       "0                                                0.0                                                                                                  \n",
       "1                                                0.0                                                                                                  \n",
       "2                                                0.0                                                                                                  \n",
       "3                                                0.0                                                                                                  \n",
       "4                                                1.0                                                                                                  \n",
       "\n",
       "   no_stopwords_show amazing , fresh & innovative idea 70 's first aired . first 7 8 years brilliant , things dropped . 1990 , show really funny anymore , 's continued decline complete waste time today. < br / > < br / > 's truly disgraceful far show fallen . writing painfully bad , performances almost bad - mildly entertaining respite guest-hosts , show probably would n't still air . find hard believe creator hand-selected original cast also chose band hacks followed . one recognize brilliance see fit replace mediocrity ? felt must give 2 stars respect original cast made show huge success . , show awful . ca n't believe 's still air .  \\\n",
       "0                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "1                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "2                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "3                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "\n",
       "   no_stopwords_sure would like see resurrection dated Seahunt series tech today would bring back kid excitement me.I grew black white TV Seahunt Gunsmoke hero 's every week.You vote comeback new sea hunt.We need change pace TV would work world water adventure.Oh way thank outlet like view many viewpoints TV many movies.So ole way believe 've got wan na say.Would nice read plus points sea hunt.If rhymes would 10 lines would let submit , leave doubt quit , must go lets .  \n",
       "0                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1                                                1.0                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "2                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "3                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                        "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "350cd137-4d4c-44d3-b19e-14ea7cd31235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>1990</th>\n",
       "      <th>25</th>\n",
       "      <th>70</th>\n",
       "      <th>950</th>\n",
       "      <th>acting</th>\n",
       "      <th>adventure</th>\n",
       "      <th>air</th>\n",
       "      <th>aired</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worst</th>\n",
       "      <th>would</th>\n",
       "      <th>wrenching</th>\n",
       "      <th>writing</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  15  1990  25  70  950  acting  adventure  air  aired  ...  white  work  \\\n",
       "0   0   1     0   1   0    0       0          0    0      0  ...      0     0   \n",
       "1   1   0     0   0   0    0       0          1    0      0  ...      1     1   \n",
       "2   0   0     1   0   1    0       0          0    2      1  ...      0     0   \n",
       "3   0   0     0   0   0    1       1          0    0      0  ...      0     0   \n",
       "4   0   0     0   0   0    0       0          0    0      0  ...      0     0   \n",
       "\n",
       "   world  worst  would  wrenching  writing  years  you  young  \n",
       "0      1      0      0          0        0      1    0      0  \n",
       "1      1      0      6          0        0      0    1      0  \n",
       "2      0      0      1          0        1      1    0      0  \n",
       "3      0      1      0          0        0      0    0      0  \n",
       "4      0      0      0          1        0      0    0      1  \n",
       "\n",
       "[5 rows x 241 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aafa05c1-bf6d-440e-a5dd-c569f6aad562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>1990</th>\n",
       "      <th>25</th>\n",
       "      <th>70</th>\n",
       "      <th>950</th>\n",
       "      <th>acting</th>\n",
       "      <th>adventure</th>\n",
       "      <th>air</th>\n",
       "      <th>aired</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worst</th>\n",
       "      <th>would</th>\n",
       "      <th>wrenching</th>\n",
       "      <th>writing</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  15  1990  25  70  950  acting  adventure  air  aired  ...  white  work  \\\n",
       "0   0   1     0   1   0    0       0          0    0      0  ...      0     0   \n",
       "1   1   0     0   0   0    0       0          1    0      0  ...      1     1   \n",
       "2   0   0     1   0   1    0       0          0    2      1  ...      0     0   \n",
       "3   0   0     0   0   0    1       1          0    0      0  ...      0     0   \n",
       "4   0   0     0   0   0    0       0          0    0      0  ...      0     0   \n",
       "\n",
       "   world  worst  would  wrenching  writing  years  you  young  \n",
       "0      1      0      0          0        0      1    0      0  \n",
       "1      1      0      6          0        0      0    1      0  \n",
       "2      0      0      1          0        1      1    0      0  \n",
       "3      0      1      0          0        0      0    0      0  \n",
       "4      0      0      0          1        0      0    0      1  \n",
       "\n",
       "[5 rows x 241 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe9173fc-9294-442c-b2f3-a16de2138b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 lines</th>\n",
       "      <th>15 times</th>\n",
       "      <th>1990 show</th>\n",
       "      <th>25 years</th>\n",
       "      <th>70 first</th>\n",
       "      <th>950 films</th>\n",
       "      <th>acting soundtrack</th>\n",
       "      <th>adventure oh</th>\n",
       "      <th>air find</th>\n",
       "      <th>aired first</th>\n",
       "      <th>...</th>\n",
       "      <th>would like</th>\n",
       "      <th>would nice</th>\n",
       "      <th>would still</th>\n",
       "      <th>would work</th>\n",
       "      <th>wrenching laughter</th>\n",
       "      <th>writing painfully</th>\n",
       "      <th>years brilliant</th>\n",
       "      <th>years paul</th>\n",
       "      <th>you vote</th>\n",
       "      <th>young old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10 lines  15 times  1990 show  25 years  70 first  950 films  \\\n",
       "0         0         1          0         1         0          0   \n",
       "1         1         0          0         0         0          0   \n",
       "2         0         0          1         0         1          0   \n",
       "3         0         0          0         0         0          1   \n",
       "4         0         0          0         0         0          0   \n",
       "\n",
       "   acting soundtrack  adventure oh  air find  aired first  ...  would like  \\\n",
       "0                  0             0         0            0  ...           0   \n",
       "1                  0             1         0            0  ...           1   \n",
       "2                  0             0         1            1  ...           0   \n",
       "3                  1             0         0            0  ...           0   \n",
       "4                  0             0         0            0  ...           0   \n",
       "\n",
       "   would nice  would still  would work  wrenching laughter  writing painfully  \\\n",
       "0           0            0           0                   0                  0   \n",
       "1           1            0           1                   0                  0   \n",
       "2           0            1           0                   0                  1   \n",
       "3           0            0           0                   0                  0   \n",
       "4           0            0           0                   1                  0   \n",
       "\n",
       "   years brilliant  years paul  you vote  young old  \n",
       "0                0           1         0          0  \n",
       "1                0           0         1          0  \n",
       "2                1           0         0          0  \n",
       "3                0           0         0          0  \n",
       "4                0           0         0          1  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ead67e06-b541-401a-90c2-135a49d14930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>10 lines</th>\n",
       "      <th>10 lines would</th>\n",
       "      <th>15</th>\n",
       "      <th>15 times</th>\n",
       "      <th>15 times last</th>\n",
       "      <th>1990</th>\n",
       "      <th>1990 show</th>\n",
       "      <th>1990 show really</th>\n",
       "      <th>25</th>\n",
       "      <th>...</th>\n",
       "      <th>years brilliant</th>\n",
       "      <th>years brilliant things</th>\n",
       "      <th>years paul</th>\n",
       "      <th>years paul lukas</th>\n",
       "      <th>you</th>\n",
       "      <th>you vote</th>\n",
       "      <th>you vote comeback</th>\n",
       "      <th>young</th>\n",
       "      <th>young old</th>\n",
       "      <th>young old love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 841 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  10 lines  10 lines would  15  15 times  15 times last  1990  1990 show  \\\n",
       "0   0         0               0   1         1              1     0          0   \n",
       "1   1         1               1   0         0              0     0          0   \n",
       "2   0         0               0   0         0              0     1          1   \n",
       "3   0         0               0   0         0              0     0          0   \n",
       "4   0         0               0   0         0              0     0          0   \n",
       "\n",
       "   1990 show really  25  ...  years brilliant  years brilliant things  \\\n",
       "0                 0   1  ...                0                       0   \n",
       "1                 0   0  ...                0                       0   \n",
       "2                 1   0  ...                1                       1   \n",
       "3                 0   0  ...                0                       0   \n",
       "4                 0   0  ...                0                       0   \n",
       "\n",
       "   years paul  years paul lukas  you  you vote  you vote comeback  young  \\\n",
       "0           1                 1    0         0                  0      0   \n",
       "1           0                 0    1         1                  1      0   \n",
       "2           0                 0    0         0                  0      0   \n",
       "3           0                 0    0         0                  0      0   \n",
       "4           0                 0    0         0                  0      1   \n",
       "\n",
       "   young old  young old love  \n",
       "0          0               0  \n",
       "1          0               0  \n",
       "2          0               0  \n",
       "3          0               0  \n",
       "4          1               1  \n",
       "\n",
       "[5 rows x 841 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0139a-9495-4be8-a233-8c41c740c5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5db06f76-da8c-452f-bcda-8a68e220843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words Removal:\n",
      " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'Artificial', 'Intelligence', '.', 'NLP', 'involves', 'analyzing', ',', 'understanding', ',', 'generating', 'languages', 'humans', 'use', 'naturally', '.']\n",
      "\n",
      "Word Tokenization:\n",
      " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', '.', 'NLP', 'involves', 'analyzing', ',', 'understanding', ',', 'and', 'generating', 'languages', 'that', 'humans', 'use', 'naturally', '.']\n",
      "\n",
      "Sentence Tokenization:\n",
      " ['Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.', 'NLP involves analyzing, understanding, and generating languages that humans use naturally.']\n",
      "\n",
      "Stemming:\n",
      " ['natur', 'languag', 'process', '(', 'nlp', ')', 'fascin', 'field', 'artifici', 'intellig', '.', 'nlp', 'involv', 'analyz', ',', 'understand', ',', 'gener', 'languag', 'human', 'use', 'natur', '.']\n",
      "\n",
      "Lemmatization:\n",
      " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'Artificial', 'Intelligence', '.', 'NLP', 'involves', 'analyzing', ',', 'understanding', ',', 'generating', 'language', 'human', 'use', 'naturally', '.']\n",
      "\n",
      "POS Tagging:\n",
      " [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('fascinating', 'VBG'), ('field', 'NN'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('.', '.'), ('NLP', 'NNP'), ('involves', 'VBZ'), ('analyzing', 'VBG'), (',', ','), ('understanding', 'VBG'), (',', ','), ('generating', 'VBG'), ('languages', 'NNS'), ('humans', 'NNS'), ('use', 'VBP'), ('naturally', 'RB'), ('.', '.')]\n",
      "\n",
      "TF-IDF Vectorization:\n",
      " [[0.         0.         0.32433627 0.32433627 0.32433627 0.\n",
      "  0.         0.32433627 0.         0.32433627 0.32433627 0.\n",
      "  0.32433627 0.         0.23076793 0.32433627 0.32433627 0.\n",
      "  0.         0.        ]\n",
      " [0.30851498 0.30851498 0.         0.         0.         0.30851498\n",
      "  0.30851498 0.         0.30851498 0.         0.         0.30851498\n",
      "  0.         0.30851498 0.21951095 0.         0.         0.30851498\n",
      "  0.30851498 0.30851498]]\n",
      "\n",
      "TF-IDF Feature Names:\n",
      " ['analyzing' 'and' 'artificial' 'fascinating' 'field' 'generating'\n",
      " 'humans' 'intelligence' 'involves' 'is' 'language' 'languages' 'natural'\n",
      " 'naturally' 'nlp' 'of' 'processing' 'that' 'understanding' 'use']\n",
      "\n",
      "One-Hot Encoding:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Bag of Words:\n",
      " [[0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0]\n",
      " [1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1]]\n",
      "\n",
      "BOW Feature Names:\n",
      " ['analyzing' 'and' 'artificial' 'fascinating' 'field' 'generating'\n",
      " 'humans' 'intelligence' 'involves' 'is' 'language' 'languages' 'natural'\n",
      " 'naturally' 'nlp' 'of' 'processing' 'that' 'understanding' 'use']\n",
      "\n",
      "Unigram Representation:\n",
      " [[0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0]\n",
      " [1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1]]\n",
      "\n",
      "Unigram Feature Names:\n",
      " ['analyzing' 'and' 'artificial' 'fascinating' 'field' 'generating'\n",
      " 'humans' 'intelligence' 'involves' 'is' 'language' 'languages' 'natural'\n",
      " 'naturally' 'nlp' 'of' 'processing' 'that' 'understanding' 'use']\n",
      "\n",
      "Bigram Representation:\n",
      " [[0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0]\n",
      " [1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1]]\n",
      "\n",
      "Bigram Feature Names:\n",
      " ['analyzing understanding' 'and generating' 'artificial intelligence'\n",
      " 'fascinating field' 'field of' 'generating languages' 'humans use'\n",
      " 'involves analyzing' 'is fascinating' 'language processing'\n",
      " 'languages that' 'natural language' 'nlp involves' 'nlp is'\n",
      " 'of artificial' 'processing nlp' 'that humans' 'understanding and'\n",
      " 'use naturally']\n",
      "\n",
      "n-gram (3-gram) Representation:\n",
      " [[0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0]\n",
      " [1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1]]\n",
      "\n",
      "n-gram (3-gram) Feature Names:\n",
      " ['analyzing understanding and' 'and generating languages'\n",
      " 'fascinating field of' 'field of artificial' 'generating languages that'\n",
      " 'humans use naturally' 'involves analyzing understanding'\n",
      " 'is fascinating field' 'language processing nlp' 'languages that humans'\n",
      " 'natural language processing' 'nlp involves analyzing'\n",
      " 'nlp is fascinating' 'of artificial intelligence' 'processing nlp is'\n",
      " 'that humans use' 'understanding and generating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text dataset (You can replace this with any text corpus)\n",
    "text = \"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence. NLP involves analyzing, understanding, and generating languages that humans use naturally.\"\n",
    "\n",
    "# 1. Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Stop Words Removal:\\n\", filtered_tokens)\n",
    "\n",
    "# 2. Tokenization\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"\\nWord Tokenization:\\n\", word_tokens)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"\\nSentence Tokenization:\\n\", sentence_tokens)\n",
    "\n",
    "# 3. Stemming and Lemmatization\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nStemming:\\n\", stemmed_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nLemmatization:\\n\", lemmatized_tokens)\n",
    "\n",
    "# 4. POS Tagging\n",
    "pos_tags = pos_tag(filtered_tokens)\n",
    "print(\"\\nPOS Tagging:\\n\", pos_tags)\n",
    "\n",
    "# 5. TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nTF-IDF Vectorization:\\n\", tfidf_matrix.toarray())\n",
    "print(\"\\nTF-IDF Feature Names:\\n\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 6. One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_words = onehot_encoder.fit_transform(np.array(filtered_tokens).reshape(-1, 1))\n",
    "print(\"\\nOne-Hot Encoding:\\n\", encoded_words)\n",
    "\n",
    "# 7. Bag of Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nBag of Words:\\n\", bow_matrix.toarray())\n",
    "print(\"\\nBOW Feature Names:\\n\", count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 8. Unigram, Bigram, n-gram\n",
    "# Unigram\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "unigram_matrix = unigram_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nUnigram Representation:\\n\", unigram_matrix.toarray())\n",
    "print(\"\\nUnigram Feature Names:\\n\", unigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Bigram\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nBigram Representation:\\n\", bigram_matrix.toarray())\n",
    "print(\"\\nBigram Feature Names:\\n\", bigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# n-gram (3-gram)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nn-gram (3-gram) Representation:\\n\", ngram_matrix.toarray())\n",
    "print(\"\\nn-gram (3-gram) Feature Names:\\n\", ngram_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c89fbf-7262-4c58-a9e8-baa2baba94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load the IMDB dataset (you may need to change the path if necessary)\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Select the text column ('review') and label column ('sentiment')\n",
    "texts = df['review']\n",
    "labels = df['sentiment']\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Stop words removal\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "texts = texts.apply(preprocess_text)\n",
    "\n",
    "# Example: Using the first review for demonstration\n",
    "text = texts[0]\n",
    "\n",
    "# 1. Stop Words Removal (already done in preprocess_text)\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Stop Words Removal:\\n\", tokens)\n",
    "\n",
    "# 2. Tokenization\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"\\nWord Tokenization:\\n\", word_tokens)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"\\nSentence Tokenization:\\n\", sentence_tokens)\n",
    "\n",
    "# 3. Stemming and Lemmatization (already done in preprocess_text)\n",
    "# Stemming and Lemmatization are combined in preprocess_text, so the tokens are already stemmed and lemmatized.\n",
    "\n",
    "# 4. POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tagging:\\n\", pos_tags)\n",
    "\n",
    "# 5. TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nTF-IDF Vectorization:\\n\", tfidf_matrix.toarray())\n",
    "print(\"\\nTF-IDF Feature Names:\\n\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 6. One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_words = onehot_encoder.fit_transform(np.array(tokens).reshape(-1, 1))\n",
    "print(\"\\nOne-Hot Encoding:\\n\", encoded_words)\n",
    "\n",
    "# 7. Bag of Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nBag of Words:\\n\", bow_matrix.toarray())\n",
    "print(\"\\nBOW Feature Names:\\n\", count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 8. Unigram, Bigram, n-gram\n",
    "# Unigram\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "unigram_matrix = unigram_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nUnigram Representation:\\n\", unigram_matrix.toarray())\n",
    "print(\"\\nUnigram Feature Names:\\n\", unigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Bigram\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_matrix = bigram_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nBigram Representation:\\n\", bigram_matrix.toarray())\n",
    "print(\"\\nBigram Feature Names:\\n\", bigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# n-gram (3-gram)\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(sentence_tokens)\n",
    "print(\"\\nn-gram (3-gram) Representation:\\n\", ngram_matrix.toarray())\n",
    "print(\"\\nn-gram (3-gram) Feature Names:\\n\", ngram_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07e06a-762c-49eb-b90a-ca83e5c355e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
